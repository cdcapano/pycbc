#!/usr/bin/env python

__prog__ = 'pycbc_calc_exval'
__author__ = 'Collin Capano <collin.capano@ligo.org>'
__description__ = \
"""Calculates SNR, and chisq values for a set of signals given a set of
injections with matching templates. This will be done for every map found
between injections in a sim_inspiral table and templates in a sngl_inspiral
table. Values are stored for the best match, determined by SNR and New SNR."""

import sqlite3
import numpy
import os, sys, shutil
import time
import random
import pickle
import operator
from optparse import OptionParser

import lal
import lalsimulation as lalsim

from glue.ligolw import utils
from glue.ligolw import table
from glue.ligolw import lsctables
from glue.ligolw import dbtables
from glue.ligolw.utils import process

from pylal import ligolw_sqlutils as sqlutils

from pycbc import types as pytypes
from pycbc import filter
from pycbc import noise
from pycbc import vetoes
from pycbc.overlaps import waveform_utils, overlap_utils


parser = OptionParser(description = __description__)
parser.add_option('-o', '--output-dir', default = '.', help = 'Directory to save overlap output data. Default is current.')
parser.add_option("-t", "--tmp-space", action = "store", type = "string", default = None, metavar = "PATH", help = "Location of local disk on which to do work. This is used to enhance performance in a networked environment.")
parser.add_option("-p", "--psd-model", help = "What PSD model to use for overlaps. Options are %s." %(', '.join(overlap_utils.get_psd_models())))
parser.add_option("-A", "--asd-file", help = "Load the PSD from a dat file containing an ASD. If both this and psd-model specified, this will take precedence.")
parser.add_option('-f', '--waveform-f-min', type = 'float', help = "Frequency at which to start the waveform generation (in Hz).")
parser.add_option('-F', '--overlap-f-min', type = 'float', help = "Frequency at which to start the overlap (in Hz). Note: This must be larger than waveform-f-min.")
parser.add_option('-r', '--sample-rate', type = 'int', help = "Required. Sample rate to use (in Hz). Must be a power of 2. If vary-n is turned on and the sample rate used to generate the waveforms and is less than this, the effectulaness, snr, and chisq will be found by zero-padding the frequency-domain such that the time series will have this sample rate. If the sample rate used in the waveform generation is greater than this, that sample rate will be used.")
parser.add_option('-l', '--segment-length', type = 'int', help = "Segment length to use (in s). Must be a power of 2. Required if vary-n not used.")
parser.add_option('-N', '--vary-n', action='store_true', default=False, help='If turned on, the sample rate and segment length used will be set to the smallest possible value between a given template and injection pair.')
parser.add_option('-n', '--n-realizations', type = 'int', help = "Number of noise realizations to generate to calculate the mean snr, chisq, and new-snr.")
parser.add_option('-i', '--ifo', metavar='IFO1[,IFO2,...]', help="What ifo(s) to inject into. If none specified, no response function will be applied to the injections. If multiple specified, combined new snr, snr, and chisq will be calculated and stored, with the single detector results saved to their own table. No coincidence window is used: two injections are counted as coincident as long as they create triggers in a realization that have SNRs > than the loudest trigger in the same realization without injections, regardless of the time difference between the triggers. Note that the same PSD is used for all the ifos.")
parser.add_option('-U', '--user-tag', help = "User tag to add to all output files.")
parser.add_option('-a', '--approximant', help = "Approximant to use for the templates.")
parser.add_option("", "--amp-order", type=int, default=0, help="Amplitude order to use for templates. Default is 0.")
parser.add_option("", "--phase-order", type=int, default=7, help="Phase order to use for templates. Default is 7 (3.5PN).")
parser.add_option("", "--spin-order", type=int, default=None, help="Spin order to use for templates.")
parser.add_option("", "--taper", help = "For TD approximants, set whether or not to taper the templates at the start and/or end. Options are: start, end, start_end. If not specified, no tapering will be done.")
parser.add_option('-b', '--n-chisq-bins', type = 'int', help = 'Number of chisq bins to use.')
parser.add_option('-M', '--use-waveform-cache', metavar = '"memory" or "disk"', help = "Use a cache to save waveforms. This will speed up computation time if you have the same injection mapped to multiple templates, or the same template mapped to multiple injections. Options are 'memory' or 'disk'. If 'memory', all waveforms will be stored in memory. If 'disk', waveforms will be stored to a temporary h5py archive. If a temp-space is specified, this temporary archive will be placed there; otherwise, it will be created in the current working directory.")
parser.add_option('-T', '--checkpoint-dt', type = 'float', default = numpy.inf, help = 'Number of minutes between checkpoints. Default is infinity, meaning no checkpointing.')
parser.add_option('', '--checkpoint-window', type='float', default=0., help='To prevent all jobs writing to disk at the same time, this setting will randomize the times between checkpoints to checkpoint-dt + w, where w is chosen uniformly in the range [-checkpoint-window, checkpoint-window]. Must be less than checkpoint-dt. Default is 0.') 
parser.add_option('-R', '--replace', action='store_true', default=False, help = 'If the output database already exists, overwrite it. Default action is not to overwrite.')
parser.add_option('-X', '--overwrite-results', action = 'store_true', default = False, help = 'If snr, chisq, and new snr already exist in the database, overwrite them. Otherwise, results with these values populated will be skipped. Note that turning this option on effectively turns off checkpointing.')
parser.add_option('-S', '--seed', type = 'int', default = int((time.time()*100)% 1e6), help = 'Set the seed to use for the gaussian noise generator. If none specified, will use the current time.')
parser.add_option('-v', '--verbose', action = 'store_true', help = 'Be verbose.')

opts, filenames = parser.parse_args()

# check options
# set checkpoint window
if opts.checkpoint_dt < opts.checkpoint_window:
    raise ValueError("checkpoint-dt must be greater than checkpoint-window")
checkpoint_dt = opts.checkpoint_dt + \
    numpy.random.uniform(-opts.checkpoint_window, opts.checkpoint_window)

ifos = []
if opts.ifo is not None:
    for ifo in opts.ifo.split(','):
        ifo = ifo.upper()
        if len(ifo) != 2:
            raise ValueError("--ifo must be of format {site}{number}[,etc.], "+
                "e.g., 'H1[,L1]'; see help")
        ifos.append(ifo)
else:
    ifos = [None]

# we'll add an additional table to store single ifo info
# if there is more than one ifo
if len(ifos) > 1:
    sngl_ifo_table = 'sngl_ifo_overlap_results'
wfmin = opts.waveform_f_min
ofmin = opts.overlap_f_min
if ofmin < wfmin:
    raise ValueError, "--waveform-f-min must be less than overlap-f-min"

if opts.sample_rate is None:
    raise ValueError("must specify a sample-rate")
target_sample_rate = opts.sample_rate
if numpy.log2(target_sample_rate) % 1 != 0.:
    raise ValueError("--sample-rate must be a power of 2")

if not opts.vary_n:
    sample_rate = target_sample_rate
    if opts.segment_length is None:
        raise ValueError("if not --vary-n, must specify a segment-length")
    seg_length = opts.segment_length
    if seg_length is None or numpy.log2(seg_length) % 1 != 0.:
        raise ValueError("--segment-length must be a power of 2")
    N = sample_rate * seg_length
    df = 1./seg_length

if opts.asd_file is None and opts.psd_model is None:
    raise ValueError("must specify --asd-file or --psd-model")
if opts.asd_file is not None and not os.path.exists(opts.asd_file):
    raise ValueError("asd-file %s not found" % opts.asd_file)

if not opts.n_realizations:
    raise ValueError, "--n-realizations required"

taper = waveform_utils.get_taper_string(opts.taper)

# create a waveform archive if desired
if opts.use_waveform_cache == 'disk':
    archive = waveform_utils.get_scratch_archive(
        opts.tmp_space is not None and opts.tmp_space or '.')
else:
    archive = {}

# set the start time
last_time = time.time()

# initialize the workspace
work_space = overlap_utils.WorkSpace()

# get the psd model to use
if opts.asd_file is None:
    psd_model = opts.psd_model

for fnum, infile in enumerate(filenames):

    if opts.verbose:
        print >> sys.stdout, "Analyzing file %s" % infile
    if not os.path.exists(infile):
        raise ValueError, "the input file %s could not be found" % infile

    outfile = overlap_utils.get_exval_outfilename(opts.output_dir, opts.ifo,
        opts.user_tag, fnum)
    if outfile == infile:
        raise ValueError, "output file (%s) is the same " % outfile +\
            "as the input file (%s)" %(infile)

    if opts.tmp_space:
        if os.path.exists(outfile) and not opts.replace:
            working_filename = dbtables.get_connection_filename(outfile,
                tmp_path = opts.tmp_space, verbose = opts.verbose)
        else:
            working_filename = dbtables.get_connection_filename(infile,
                tmp_path = opts.tmp_space, verbose = opts.verbose)
        connection = sqlite3.connect(working_filename)
        dbtables.set_temp_store_directory(connection, opts.tmp_space,
            verbose=opts.verbose)
    else:
        connection = overlap_utils.get_outfile_connection(infile, outfile,
            replace=opts.replace, verbose=opts.verbose)

    # FIXME: dbtables apparently has a problem with overlap_results table
    try:
        xmldoc = dbtables.get_xml(connection)
        this_process = process.register_to_xmldoc(xmldoc, __prog__,
            opts.__dict__).process_id
    except:
        print >> sys.stderr, "Warning: not writing process info to database"
        this_process = sqlutils.get_next_id(connection, 'process',
            'process_id')

    # get the injections and matching templates
    if opts.verbose:
        print >> sys.stdout, "Getting injections and matching templates..."
    inj_tmplt_map = overlap_utils.get_injection_template_map(connection)
    injections = waveform_utils.InjectionDict()
    injections.get_injections(connection, wfmin, archive=archive,
        calc_f_final=opts.vary_n, estimate_dur=True,
        verbose=opts.verbose)
    templates = waveform_utils.TemplateDict()
    templates.get_templates(connection, opts.approximant, wfmin,
        amp_order=opts.amp_order, phase_order=opts.phase_order,
        spin_order=opts.spin_order, taper=taper,
        archive=archive, calc_f_final=True, estimate_dur=True,
        verbose=opts.verbose, only_matching=True)

    # if we're overwriting results, easiest thing to do is to just drop the
    # results table, since we'll next create one if it doesn't exist
    if opts.overwrite_results:
        connection.cursor().execute('DROP TABLE IF EXISTS overlap_results') 
        if len(ifos) > 1:
            connection.cursor().execute(
                'DROP TABLE IF EXISTS %s' % sngl_ifo_table)

    # create the overlap results table if it doesn't exist yet 
    overlap_utils.create_results_table(connection)
    # also create a sngl_ifo_overlap_results table if we have more than 1 ifo
    if len(ifos) > 1:
        overlap_utils.create_results_table(connection, sngl_ifo_table,
            set_primary_key=False)

    # get results that have already been analyzed: these are results
    # where the snr, chisq and new_snr are filled in
    # this is done by first deleting any results in the overlaps_results
    # table that do not have these values filled in, then getting the list
    # of coinc_event_ids that remain. This way, if we are calculating
    # expectation values from the output of a pycbc_overlaps job, all the
    # results will be written over.
    sqlquery = """
        DELETE FROM
            overlap_results
        WHERE
            snr IS NULL AND
            chisq IS NULL AND
            new_snr IS NULL
        """
    connection.cursor().execute(sqlquery)

    already_analyzed = [ceid for (ceid,) in connection.cursor().execute(
        'SELECT coinc_event_id FROM overlap_results')]

    # remove maps from the inj_tmplt_map table that were already analyzed
    if opts.verbose:
        skip_num = len(already_analyzed)
        print >> sys.stdout, "Skipping %i injections" % skip_num 
    for ceid in already_analyzed:
        inj_tmplt_map.pop(ceid)

    #
    #   Cycle over the results, calculating the expectation values for each
    #
    for ii, (ceid, (simid, tmpltid)) in enumerate(inj_tmplt_map.items()):

        if opts.verbose:
            print >> sys.stdout, "Injection %i\r" % (ii+1+skip_num),
            sys.stdout.flush()
       
        inj = injections[simid]
        tmplt = templates[tmpltid]

        # get the sample rate and segment length to use
        if opts.vary_n:
            sample_rate = min(
                max(tmplt.min_sample_rate, inj.min_sample_rate),
                target_sample_rate)

            # get the segment length to use
            # we'll set the segment length to be four times the max duration
            # of this template and the injection
            seg_length = int(2**(numpy.ceil(numpy.log2(
                max(tmplt.duration, inj.duration)))+2))
            # since the duration estimates are based on the 2PN
            # approximation, the duration may be too short; we'll just put
            # a floor at 4s
            if seg_length < 4:
                seg_length = 4

            df = 1./seg_length

        # get the template waveform
        htilde = tmplt.get_fd_waveform(sample_rate, seg_length,
            store=opts.use_waveform_cache is not None,
            resample_from_max=opts.vary_n)

        sngl_snrs = {}
        sngl_chisq_vals = {}
        sngl_newsnrs = {}
        sngl_success_idx = {}
        for ifo in ifos:
            # set the segment start time; we'll always make this so
            # that the injection ends ~ 5/8 into the segment; the following
            # works like so: since the injection duration is ~1/4 the segment
            # length, putting the start time 3/8 into the segment will cause
            # it to end at ~5/8. We choose 5/8 because we will throw out the
            # first and last quarter of the segment later on, but we want
            # to allow some slop because the duration is only an estimate
            segment_start = inj.detector_end_time(ifo) - inj.duration \
                - 3*seg_length/8.

            # get the FD injection and waveform
            # Note that these will also store the TD versions if they don't
            # exist in the archive
            htildeprime = inj.get_fd_waveform(sample_rate, seg_length,
                ifo, segment_start, store=opts.use_waveform_cache is not None,
                resample_from_max=opts.vary_n)

            # get psd and needed workspace vectors
            if opts.asd_file is not None:
                psd = work_space.get_psd_from_file(wfmin, sample_rate,
                    seg_length, opts.asd_file, dyn_range_exp=0)
            else:
                psd = work_space.get_psd(wfmin, sample_rate, seg_length,
                    psd_model, dyn_range_exp=0)

            # we'll need a place to store work space vectors;
            # note that the length of these are set to the
            # target_sample_rate * the segment length
            N = target_sample_rate * seg_length
            kmax = N/2+1
            snr_work_mem = pytypes.zeros(N,
                dtype=pytypes.complex_same_precision_as(htilde))
            corr_work_mem = pytypes.zeros(N,
                dtype=pytypes.complex_same_precision_as(htilde))
            work_htilde = pytypes.FrequencySeries(pytypes.zeros(kmax,
                dtype=pytypes.complex_same_precision_as(htilde)),
                delta_f=htilde.delta_f)
            work_stilde = pytypes.FrequencySeries(pytypes.zeros(kmax,
                dtype=pytypes.complex_same_precision_as(htildeprime)),
                delta_f=htildeprime.delta_f)
            work_psd = pytypes.FrequencySeries(pytypes.ones(kmax,
                dtype=pytypes.real_same_precision_as(psd)),
                delta_f=psd.delta_f)

            # get the chisq bins
            chisq_bins = vetoes.power_chisq_bins(htilde, opts.n_chisq_bins,
                psd, ofmin)

            # compute the SNR. We do this using filter_by_padding. If
            # the sample_rate is equal to the target_sample_rate, than this
            # will essentially just call filter.matched_filter_core.
            cmplx_ts, corr, norm = overlap_utils.filter_by_padding(htilde,
                htildeprime, psd, ofmin, target_sample_rate,
                work_v1=work_htilde, work_v2=work_stilde, work_psd=work_psd,
                out=snr_work_mem, corr_out=corr_work_mem)
            # normalize and drop the first and last quarter of the time series;
            # we don't really need to drop the first and last quarter for this,
            # it will just speed up calculations later on when computing the
            # snr in noise
            cmplx_ts = cmplx_ts[N/4:3*N/4]*norm
            sigmaprime = filter.sigma(htildeprime, psd, ofmin)

            maxidx = abs(cmplx_ts).data.argmax()
            effectualness = abs(cmplx_ts[maxidx])/sigmaprime
            offset = (maxidx + N/4) * cmplx_ts.delta_t

            time_offset = segment_start + offset - inj.geocent_time

            # cycle over the desired number of sample times, calculating snr,
            # chisq, and newsnr for each
            sngl_snrs[ifo] = numpy.zeros(opts.n_realizations)
            sngl_chisq_vals[ifo] = numpy.zeros(opts.n_realizations)
            sngl_newsnrs[ifo] = numpy.zeros(opts.n_realizations)
            sngl_success_idx[ifo] = numpy.empty(0, dtype=int)
            for nn in range(opts.n_realizations):
                # create an instance of the noise
                # pycbc's noise package will give the same noise if you pass
                # the seed argument everytime. We therefore only pass it a
                # seed on the first pass.
                if nn == 0:
                    seed = opts.seed
                else:
                    seed = None
                ntilde = noise.frequency_noise_from_psd(psd, seed=seed)

                # check what the loudest event in noise is
                snr_work_mem = pytypes.zeros(N,
                    dtype=pytypes.complex_same_precision_as(htilde))
                corr_work_mem = pytypes.zeros(N,
                    dtype=pytypes.complex_same_precision_as(htilde))
                rand_ts, rand_corr, rand_norm = \
                    overlap_utils.filter_by_padding(
                        htilde, ntilde, psd, ofmin, target_sample_rate,
                        work_v1=work_htilde, work_v2=work_stilde,
                        work_psd=work_psd, out=snr_work_mem,
                        corr_out=corr_work_mem)

                # Note: norm and rand_norm should be the same, so we'll just
                # use norm for the normalization
                rand_ts = rand_ts[N/4:3*N/4] * norm
                randsnr = abs(rand_ts).max()


                # the total snr is the sum of the complex time series from the
                # noise and from the injection
                rand_ts._epoch = cmplx_ts._epoch
                snr_ts = cmplx_ts + rand_ts
                maxidx = abs(snr_ts).data.argmax()
                snr = abs(snr_ts[maxidx])

                # only calculate chisq if the snr is louder than noise
                if snr > randsnr:
                    # the corr to use is just rand_corr + injection corr
                    chisq_corr = rand_corr + corr
                    # Note: the indx of the maximum has to be with respect to
                    # the full time series, not to the cropped time series
                    chisq_indx = pytypes.Array(numpy.array([maxidx+N/4]))
                    chisq = vetoes.power_chisq_at_points_from_precomputed(
                        chisq_corr, numpy.array([snr/norm]),
                        norm, chisq_bins, chisq_indx)[0]

                    newsnr = overlap_utils.new_snr(snr, chisq,
                        2*opts.n_chisq_bins-2)

                    sngl_snrs[ifo][nn] = snr
                    sngl_chisq_vals[ifo][nn] = chisq
                    sngl_newsnrs[ifo][nn] = newsnr
                    sngl_success_idx[ifo] = numpy.append(
                                            sngl_success_idx[ifo], nn)


        # get coincident results
        if len(ifos) > 1:
            snrs = numpy.zeros(opts.n_realizations)
            chisq_vals = numpy.zeros(opts.n_realizations)
            newsnrs = numpy.zeros(opts.n_realizations)
            success_indices = numpy.empty(0, dtype=int)
            for nn in range(opts.n_realizations):
                # apply coincidence: need at least two detectors to have
                # non-zero snrs
                coinc_ifos = [ifo for ifo in ifos if sngl_snrs[ifo][nn] != 0]
                if len(coinc_ifos) > 1:
                    # we'll do the quadrature sum of new snr, snr; for chisq,
                    # we'll just take the mean (I guess; we don't actually
                    # find coincident chisq in the real search
                    coinc_snrs = numpy.array([sngl_snrs[ifo][nn]
                        for ifo in coinc_ifos])
                    coinc_newsnrs = numpy.array([sngl_newsnrs[ifo][nn]
                        for ifo in coinc_ifos])
                    coinc_chisq_vals = numpy.array([sngl_chisq_vals[ifo][nn]
                        for ifo in coinc_ifos])
                    snrs[nn] = numpy.sqrt((coinc_snrs**2.).sum())
                    newsnrs[nn] = numpy.sqrt((coinc_newsnrs**2.).sum())
                    chisq_vals[nn] = coinc_chisq_vals.mean()
                    success_indices = numpy.append(success_indices, nn)
        else:
            # for single ifo, just store the values
            snrs = sngl_snrs[ifos[0]]
            newsnrs = sngl_newsnrs[ifos[0]]
            chisq_vals = sngl_chisq_vals[ifos[0]]
            success_indices = sngl_success_idx[ifos[0]] 


        # store the results
        this_result = overlap_utils.OverlapResult(tmplt, inj)
        this_result.segment_length = seg_length
        this_result.sample_rate = int(1./cmplx_ts.delta_t)
        this_result.overlap_f_min = ofmin 
        this_result.waveform_f_min = wfmin
        this_result.tmplt_approximant = opts.approximant 
        this_result.effectualness = effectualness
        this_result.time_offset = time_offset.gpsSeconds
        this_result.time_offset_ns = time_offset.gpsNanoSeconds
        if ifos != [None]:
            this_result.ifo = ','.join(sorted(ifos))
        # we'll only take the average over the successful attempts 
        this_result.num_tries = opts.n_realizations
        this_result.num_successes = success_indices.shape[0]
        if this_result.num_successes == 0:
            # add one index, so we'll get 0's for the means, instead of nans
            success_indices = numpy.array([0])
        this_result.snr = snrs[success_indices].mean()
        this_result.snr_std = snrs[success_indices].std()
        this_result.chisq = chisq_vals[success_indices].mean()
        this_result.chisq_std = chisq_vals[success_indices].std()
        this_result.chisq_dof = 2*opts.n_chisq_bins - 2
        this_result.new_snr = newsnrs[success_indices].mean()
        this_result.new_snr_std = newsnrs[success_indices].std()

        # dump to output database
        this_result.write_to_database(connection, ceid)

        # add the single ifo info if we have more than one ifo
        if len(ifos) > 1:
            for ifo in ifos:
                this_result = overlap_utils.OverlapResult(tmplt, inj)
                this_result.ifo = ifo 
                this_result.segment_length = seg_length
                this_result.sample_rate = int(1./cmplx_ts.delta_t)
                this_result.overlap_f_min = ofmin 
                this_result.waveform_f_min = wfmin
                this_result.tmplt_approximant = opts.approximant 
                this_result.effectualness = effectualness
                this_result.time_offset = time_offset.gpsSeconds
                this_result.time_offset_ns = time_offset.gpsNanoSeconds
                keep_idx = sngl_success_idx[ifo]
                this_result.num_tries = opts.n_realizations
                this_result.num_successes = keep_idx.shape[0]
                if this_result.num_successes == 0:
                    # add one index, so we'll get 0's for the means,
                    # instead of nans
                    keep_idx = numpy.array([0])
                this_result.snr = sngl_snrs[ifo][keep_idx].mean()
                this_result.snr_std = sngl_snrs[ifo][keep_idx].std()
                this_result.chisq = sngl_chisq_vals[ifo][keep_idx].mean()
                this_result.chisq_std = sngl_chisq_vals[ifo][keep_idx].std()
                this_result.chisq_dof = 2*opts.n_chisq_bins - 2
                this_result.new_snr = sngl_newsnrs[ifo][keep_idx].mean()
                this_result.new_snr_std = sngl_newsnrs[ifo][keep_idx].std()
                this_result.write_to_database(connection, ceid, sngl_ifo_table)

        connection.commit()

        # check time and update checkpoints if needed
        now = time.time()
        if (now - last_time)/60. > checkpoint_dt and \
                opts.tmp_space is not None:
            if opts.verbose:
                print "\ncheckpointing..."
            overlap_utils.copy_to_output(working_filename,
                outfile, verbose=False)
            last_time = now
            # randomize the next checkpoint time
            checkpoint_dt = opts.checkpoint_dt + numpy.random.uniform(
                -opts.checkpoint_window, opts.checkpoint_window)

    if opts.verbose:
        print >> sys.stdout, ""
        sys.stdout.flush()
    
    # close and exit 
    connection.commit()
    connection.close()
    if opts.tmp_space is not None:
        dbtables.put_connection_filename(outfile, working_filename,
            verbose=opts.verbose)

if opts.verbose:
    print >> sys.stdout, "Finished!"

sys.exit(0)
