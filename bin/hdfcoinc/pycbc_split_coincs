#!/usr/bin/env python

# Copyright (C) 2015 Alex Nitz, Collin Capano
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 3 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

import os
import h5py, argparse, numpy, pycbc.events, logging
from pycbc.io import lscarrays
from pycbc.io.hdfcoinc import lscarray_utils

#
# =============================================================================
#
#                           Helper functions 
#
# =============================================================================
#

# XXX: if the options are useful for other programs than just this, may want to move this function and some of the arguments into a from_cli method
def parse_coinc_files_arg(args):
    """Parses the coinc-files argument for the coinc-files and, if provided,
    trigger-merge files. Returns a dictionary keyed by the coinc-file names,
    with values being the list of trigger-merge files provided."""
    coinc_files = {}
    for filearg in args.coincfiles:
        fileinfo = filearg.split(':')
        if len(fileinfo) == 1:
            # no trigger merge files provided
            coinc_files[filearg] = None
        elif len(fileinfo) == 2:
            coinc_file, trigger_files = fileinfo
            coinc_files[coinc_file] = trigger_files.split(',')
        else:
            raise ValueError("coinc-files arg not formatted correctly")
    # check that the number of trigger merge files
    return coinc_files

def print_possible_fields(coinc_files, datatypes, bankhdf):
    """Given a dictionary of coinc fields and possible triggermerge files
    (the output of parse_coinc_files_arg), prints the possible fields.

    Parameters
    ----------
    coinc_files : dict
        Keys are the names of STATMAP hdf files, keys are lists of
        TRIGGERMERGE files (or None); basically, the output of
        parse_coinc_files_arg.
    datatypes : (list of) string
        One or more datatypes to retrieve from the coinc files. The possible
        fields will be taken from the common values across all of the
        datatypes.
    bankhdf : {None | string | open h5py.File}
        Path to BANKHDF file, open BANKHDF file, or similar.

    Returns
    -------
    string
        A string giving the available fields and methods; this can be printed
        to the terminal.
    """
    possible_fields = set()
    possible_methods = set()
    sngls_virtual_fields = {}
    sngls_method_fields = {}
    all_method_fields = {}
    if bankhdf is not None:
        dummy_bank = lscarray_utils.dummy_tmplt_inspiral_from_bankhdf(bankhdf)
    for coinc_file, triggermerge_files in coinc_files.items():
        # we'll need the sngl event arrays to determine what virtual fields
        # apply to the singles
        for tm in triggermerge_files:
            dummy_sev = lscarray_utils.dummy_sngl_events_from_triggermerge(tm)
            det = dummy_sev.detector[0]
            newfields = set(['%s.%s' %(det, vf) \
                for vf in dummy_sev.virtual_fields])
            try:
                sngls_virtual_fields[det] &= newfields
            except KeyError:
                sngls_virtual_fields[det] = newfields
            newfields = {'%s.%s' %(det, fn): mf
                for fn,mf in dummy_sev.method_fields.items()}
            all_method_fields.update(newfields)
            newfields = set(newfields.keys())
            try:
                sngls_method_fields[det] &= newfields
            except KeyError:
                sngls_method_fields[det] = newfields
        for datatype in datatypes:
            dummy_cev = lscarray_utils.dummy_coinc_events_from_statmap(
                coinc_file, datatype, bankhdf=bankhdf,
                triggermergehdfs=triggermerge_files)
            # have to be careful with virtual/method fields; they may only
            # apply to the sngl-detector subarrays
            newfields = set(dummy_cev.all_fieldnames) | (
                set(dummy_cev.virtual_fields) & \
                set(dummy_cev.default_virtual_fields))
            if bankhdf is not None:
                newfields |= set(dummy_cev.virtual_fields) & \
                    set(dummy_bank.virtual_fields)
            if not possible_fields:
                possible_fields = newfields
            else:
                possible_fields &= newfields
            newfields = set(dummy_cev.method_fields.keys()) & \
                set(dummy_cev.default_method_fields.keys())
            if bankhdf is not None:
                newfields |= set(dummy_cev.method_fields.keys()) & \
                    set(dummy_bank.method_fields.keys())
            if not possible_methods:
                possible_methods = newfields
            else:
                possible_methods &= newfields
            all_method_fields.update({fn: dummy_cev.method_fields[fn] \
                for fn in newfields})
    for fs in sngls_virtual_fields.values():
        possible_fields.update(fs)
    for fs in sngls_method_fields.values():
        possible_methods.update(fs)
    outstr = "Available fields:\n\t%s" %(
        '\n\t'.join(sorted(possible_fields)))
    outstr += "\nAvailable method fields and their needed arguments:\n\t%s"%(
        '\n\t'.join(sorted(['%s: %s' %(name, all_method_fields[name])
            for name in possible_methods])))
    return outstr

def get_all_dataset_paths(hdffile, skipfields=[], prefix=''):
    """Recursively searches an hdf file or a group in an hdf file for all
    dataset paths, returning the names.

    Parameters
    ----------
    hdffile : h5py.File
        The hdffile to search.
    skipfields : {[], list}
        Any groups/datasets specified will not be included in the returned
        list.
    prefix : {'', string}
        Prefix all of the returned names with the given string.

    Returns
    -------
    list
        The list of datasets, excluding the skipfields. Datasets will use
        full path names, e.g., 'foo/bar'.
    """
    if prefix != '':
        prefix += '/'
    datasets = []
    for thiskey in hdffile:
        fullname = ''.join([prefix, thiskey])
        if fullname in skipfields:
            continue
        if isinstance(hdffile[thiskey], h5py.Group):
            datasets += get_all_dataset_paths(hdffile[thiskey],
                skipfields=skipfields, prefix=fullname)
        else:
            datasets.append(fullname)
    return datasets

def copy_hdffile(infile, outfile, skipfields=[], overwrite_attrs=False):
    """Copies data from one hdf file to another. All datasets in the infile
    will searched for recursively. If the outfile already has a dataset
    with the same path as a dataset in infile, the dataset from infile will
    be appended to outfile's.

    Parameters
    ----------
    infile : h5py.File
        The hdf file from which to copy data.
    outfile : h5py.File
        The hdf file to which to copy data.
    skipfields : {[], list}
        Do not copy the specified groups/datasets.
    ovewrite_attrs : bool
        If True, will replace the outfile's attrs with the infile's attrs.
        Otherwise, an error is raised if the infile's attrs do not match the
        attrs of the outfile, and the outfile's attrs are not empty. If the
        outfile's attrs are empty, the infile's attrs are copied regardless.
    """
    # get all the datasets to copy
    dataset_paths = get_all_dataset_paths(infile, skipfields=skipfields)
    for thispath in dataset_paths:
        if thispath in outfile:
            # already exists, append
            outfile[thispath] = numpy.append(outfile[thispath],
                infile[thispath])
        else:
            group_name = os.path.dirname(thispath)
            if group_name != '' and group_name not in outfile:
                outfile.create_group(group_name)
            h5py.h5o.copy(infile.id, thispath, outfile.id, thispath)
    # copy/check that the attributes are the same
    if outfile.attrs.items() == [] or overwrite_attrs:
        for k in infile.attrs:
            outfile.attrs[k] = infile.attrs[k]
    elif outfile.attrs.items() != infile.attrs.items():
        raise ValueError("the attributes of infile and outfile do not match")

#
# =============================================================================
#
#                           Main 
#
# =============================================================================
#
parser = argparse.ArgumentParser()
parser.add_argument('--verbose', action='store_true')
parser.add_argument('coincfiles', nargs='+',
                    metavar='STATMAP_FILE[:TRIGGER_MERGE_FILE1,TRIGGER_MERGE_FILE2,...]',
                    help="List of coinc files to be redistributed. You may also specify one or more TRIGGER_MERGE file that the coinc file(s) were derived from, to get single-detector statistics.")
parser.add_argument('--bins', nargs='+',
                    help="Ordered list of bins to create. Syntax is anything that can be understood by numpy.where(). For example, '(mchirp >= 1.) & (mchirp < 2.)' will define a bin such that mchirp is in [1.,2.); 'detected_in == \"H1,L1\"' will define a bin in which triggers are detected in H1 and L1; etc. You may use any fields in the provided coinc file(s), bank-file, and/or trigger merge files (if provided), any derived fields from those, as well as any math function supported by numpy on the fields. To see the full list of available fields, use the --print-possible-fields option. No attempt is made to check that all triggers are included across all of the bins, nor that bins are mutually exclusive. If bins are not mutually exclusive, triggers that are in multiple bins will show up in each bin they qualify for.") 
parser.add_argument('--output-files', nargs='+',
                    help="list of output file names, one for each bin")
parser.add_argument('--bank-file',
                    help="Required; hdf format template bank file")
parser.add_argument('--datatypes', metavar='DATATYPE1[,DATATYPE2,...]',
                    default='background,background_exc,foreground',
                    help='What datatypes to apply the binning to. Any datatypes not listed will not be included in the output files. Default is "background, background_exc, foreground".')
parser.add_argument('-H', '--print-possible-fields', action='store_true', default=False,
                    help="Print out all possible fields the may be used for binning given the coinc-file(s), bank-file, and (if provided) trigger-merge file(s) then exit.")
args = parser.parse_args()


pycbc.init_logging(args.verbose)

# check options
if not args.print_possible_fields:
    if args.output_files is None:
        raise ValueError('output-files is required')
    if args.bins is None:
        raise ValueError('bins is required')
    if len(args.output_files) != len(args.bins):
        raise ValueError('Number of bins and output files does not match') 
if (args.bank_file is None or args.coincfiles is None):
    raise ValueError("must provide atleast one coinc-file and one bank-file")

datatypes = args.datatypes.split(',')

coinc_files = parse_coinc_files_arg(args)

if args.print_possible_fields:
    print print_possible_fields(coinc_files, datatypes, bankhdf=args.bank_file)
    exit(0)

# open the output files
outfiles = [h5py.File(outname, 'w') for outname in args.output_files]

logging.info('binning coincs')
# cycle over the coinc files, pulling out the data for each
for ii,(coinc_file, triggermerge_files) in enumerate(coinc_files.items()):
    logging.info('coinc file %i / %i' %(ii+1, len(coinc_files.keys())))
    coinchdf = h5py.File(coinc_file, 'r')
    # cycle over the datatypes to apply the bins to each type
    for datatype in datatypes:
        # construct the coinc event array with the needed fields
        coinc_events = lscarray_utils.coinc_events_from_statmap(coinchdf,
            datatype, bankhdf=args.bank_file,
            triggermergehdfs=triggermerge_files, names=args.bins)
        # cycle over the bins, getting the indices that apply
        for jj,(thisbin, outfile) in enumerate(zip(args.bins, outfiles)):
            logging.info('datatype %s: bin %i / %i' %(
                datatype, jj+1, len(args.bins)))
            if datatype not in outfile:
                outfile.create_group(datatype)
            # keepidx is a boolean array indicating which coincs to keep
            keepidx = coinc_events[thisbin]
            # copy the datatype to outfile
            for thisfield in coinchdf[datatype].keys():
                copydata = coinchdf[datatype][thisfield][keepidx]
                if thisfield in outfile[datatype]:
                    outfile[datatype][thisfield] = \
                        numpy.append(outfile[datatype][thisfield], copydata)
                else:
                    outfile[datatype][thisfield] = copydata
            logging.info('copied %i of %i triggers' %(copydata.size,
                coinc_events.size))
    # copy other data to the outfiles
    logging.info('copying auxiliary data')
    for outfile in outfiles:
        copy_hdffile(coinchdf, outfile, skipfields=datatypes,
            overwrite_attrs=False)
    # close and move on to the next
    coinchdf.close()

# close and exit
for outfile in outfiles:
    outfile.close()
